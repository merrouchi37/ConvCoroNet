{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cf7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9de149b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input, GlobalAveragePooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Optimizer, Adam,RMSprop, SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#import argparse\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bf667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the optimizer Itao ( a new Iterative thresholding algorithm based optimizer)\n",
    "class Itao(Optimizer):\n",
    "    def __init__(self, k=10.0, lamda=0.00001, name=\"Itao\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"k\", k) # tuning hyperparameter K\n",
    "        self._set_hyper(\"lamda\", lamda) # Tikhonov parameter lamda\n",
    "        \n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        \n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        k = self._get_hyper(\"k\", var_dtype)\n",
    "        lamda = self._get_hyper(\"lamda\", var_dtype)\n",
    "        new_var = (k/(k+lamda))*var - grad/(k+lamda) # update variables\n",
    "        var.assign(new_var)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"k\": self._serialize_hyperparameter(self._k),\n",
    "            \"lamda\": self._serialize_hyperparameter(self._lamda),\n",
    "            \n",
    "        }\n",
    "    def get_config(self):\n",
    "        config = super(Itao, self).get_config()\n",
    "        config.update({\n",
    "            \"k\": self._serialize_hyperparameter(\"k\"),\n",
    "            \"lamda\": self._serialize_hyperparameter(\"lamda\"),\n",
    "            \n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb233fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data and labels (the result of \"preparationOfDataset.ipynb\")\n",
    "data_train = np.load('new_3000_data_train_224.npy') \n",
    "labels_train = np.load('new_3000_labels_train_224.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97023a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c2c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling Data\n",
    "np.random.seed(12345)\n",
    "indx=np.arange(data_train.shape[0])          \n",
    "np.random.shuffle(indx)\n",
    "data_train = data_train[indx]\n",
    "labels_train = labels_train[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16240b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90dc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a model based on InceptionV3\n",
    "def create_model( ):\n",
    "    lam = l2(1e-5) # L2 regularization\n",
    "    baseModel = InceptionV3(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "    for layer in baseModel.layers:\n",
    "        if hasattr(layer, 'kernel_regularizer'):\n",
    "            setattr(layer, 'kernel_regularizer', lam)\n",
    "    x = baseModel.output\n",
    "    x = Flatten(name=\"flatten\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=lam)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(3, activation=\"softmax\", kernel_regularizer=lam)(x) \n",
    "\n",
    "    model = Model(inputs=baseModel.input, outputs=x)\n",
    "    for layer in baseModel.layers: \n",
    "        layer.trainable =  True #False\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62cbc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see trainable parameters\n",
    "#model = create_model()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a5b673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(labels_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a322a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to plot accuracy and loss curves during training and validation phases\n",
    "# Import functions to produce evaluation metrics and plot the confusion matrix\n",
    "from utils import plot_acc_loss\n",
    "from utils import evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab76332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation object\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits =5, random_state = 7, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b383ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save results and models of different folds (5 fold cross-validation)\n",
    "#!mkdir covid19_models_cross_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a23485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to save accuracy and loss for each fold\n",
    "validation_accuracy = []\n",
    "validation_loss = []\n",
    "\n",
    "base_model = \"InceptionV3\"\n",
    "save_dir = 'covid19_models_cross_val/' # A base directory to save results and models\n",
    "fold_var = 1\n",
    "for train_index, val_index in skf.split(np.zeros(len(data_train)),labels):\n",
    "  \n",
    "    training_data = data_train[train_index]\n",
    "    training_labels = labels_train[train_index]\n",
    "    validation_data = data_train[val_index]\n",
    "    validation_labels = labels_train[val_index]\n",
    "    \n",
    "    model = create_model()\n",
    "    opt = Itao()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])    \n",
    "    \n",
    "    best_model_path = save_dir + 'model_' + base_model + '_fold_' + str(fold_var) + '.h5'\n",
    "    mcp_save = ModelCheckpoint(best_model_path, save_best_only=True, monitor='val_acc', mode='max', verbose=1)\n",
    "    callbacks = [ mcp_save]\n",
    "    \n",
    "    print(\"[INFO] training K-Fold...step: \" + str(fold_var))\n",
    "    \n",
    "    histories = []\n",
    "    lesK = [ 40.0, 80.0, 120.0] # Scheduling of hyperparameter K during epochs\n",
    "    lesEpochs = [40, 40, 20]\n",
    "    BS_tr=32\n",
    "    BS_val = 32\n",
    "    for i in range(len(lesK)):\n",
    "        opt.k = lesK[i]\n",
    "        print(\"[INFO] Training model...K = \" + str(lesK[i]))\n",
    "        #model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
    "        H = model.fit(\n",
    "            trainAug.flow(training_data, training_labels, batch_size=BS_tr),\n",
    "            #steps_per_epoch=len(training_data) // BS_tr ,\n",
    "            validation_data=(validation_data, validation_labels),\n",
    "            #validation_steps=len(validation_data) // BS_val ,\n",
    "            epochs=lesEpochs[i], callbacks=callbacks,\n",
    "            shuffle=True)\n",
    "        histories.append(H.history)\n",
    "    \n",
    "    #np.save(save_dir + 'history_' + base_model + '-' + str(fold_var) + '.npy', Hs.history)\n",
    "    dict_history = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    for hist in histories:\n",
    "        for key , value in dict_history.items():\n",
    "            for a in hist[key]:\n",
    "                dict_history[key].append(a )\n",
    "    \n",
    "    plot_acc_loss(dict_history, np.sum(lesEpochs))\n",
    "\n",
    "    #Loading best model\n",
    "    model.load_weights(best_model_path)\n",
    "    #Saving weights\n",
    "    model.save_weights(save_dir + 'model_' + base_model + '_fold_' + str(fold_var) + '_weights.h5')\n",
    "    results = model.evaluate(validation_data, validation_labels)\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "    \n",
    "    validation_accuracy.append(results['acc'])\n",
    "    validation_loss.append(results['loss'])\n",
    "                       \n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "    evaluation_metrics(model, validation_data, validation_labels, fold_var, BS_val)\n",
    "    \n",
    "    #Saving accuracy and loss curves\n",
    "    np.save(save_dir + base_model + \"_fold_\" + str(fold_var) + \"_acc.npy\", dict_history[\"acc\"])\n",
    "    np.save(save_dir + base_model + \"_fold_\" + str(fold_var) + \"_loss.npy\", dict_history[\"loss\"])\n",
    "    np.save(save_dir + base_model + \"_fold_\" + str(fold_var) + \"_val_acc.npy\", dict_history[\"val_acc\"])\n",
    "    np.save(save_dir + base_model + \"_fold_\" + str(fold_var) + \"_val_loss.npy\", dict_history[\"val_loss\"])    \n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var = fold_var + 1 # Next fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43daf5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average accuracy : \", np.mean(validation_accuracy))\n",
    "print(\"Average loss : \", np.mean(validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8715e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.8 (tfdl)",
   "language": "python",
   "name": "tfdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
